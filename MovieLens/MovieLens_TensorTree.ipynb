{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/Datathon2019/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Datathon2019/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/envs/Datathon2019/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7821230253636114\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/Datathon2019/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0: te_auc: 0.7471\n",
      "1: te_auc: 0.7562\n",
      "2: te_auc: 0.7672\n",
      "3: te_auc: 0.7780\n",
      "4: te_auc: 0.7793\n",
      "5: te_auc: 0.7796\n",
      "6: te_auc: 0.7799\n",
      "7: te_auc: 0.7802\n",
      "8: te_auc: 0.7803\n",
      "9: te_auc: 0.7803\n",
      "10: te_auc: 0.7803\n",
      "11: te_auc: 0.7803\n",
      "12: te_auc: 0.7803\n",
      "13: te_auc: 0.7803\n",
      "14: te_auc: 0.7801\n",
      "15: te_auc: 0.7799\n",
      "16: te_auc: 0.7796\n",
      "17: te_auc: 0.7793\n",
      "18: te_auc: 0.7789\n",
      "19: te_auc: 0.7786\n",
      "20: te_auc: 0.7782\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from Tree_Machine import Tree_Machine, simple_batcher\n",
    "from numpy import load\n",
    "import math\n",
    "\n",
    "X_tr=load('x_train.npy')\n",
    "X_te=load('x_test.npy')\n",
    "y_tr=load('y_train.npy')\n",
    "y_te=load('y_test.npy')\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "oh = OneHotEncoder()\n",
    "oh.fit(np.vstack((X_tr, X_te))-1)\n",
    "X_tr_sp = oh.transform(X_tr-1)\n",
    "X_te_sp = oh.transform(X_te-1)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_tr_sp, y_tr)\n",
    "y_pred = logreg.predict_proba(X_te_sp)[:, 1]\n",
    "print(roc_auc_score(y_te, y_pred))\n",
    "coef = logreg.coef_[0]\n",
    "intercept = logreg.intercept_[0]\n",
    "#%%\n",
    "\n",
    "rank = 30 # has to be larger than number of features in this case \n",
    "s_features=[7,2,21,19,943,1682,10,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "\n",
    "### Initialization of factor matrices \n",
    "\n",
    "num_features = len(s_features)\n",
    "w_cores = [None] * num_features\n",
    "begin_feature = [0] + list(np.cumsum(s_features)) #### where each feature begins in the big d vector\n",
    "\n",
    "coef = logreg.coef_[0]\n",
    "intercept = logreg.intercept_[0]\n",
    "\n",
    "# MATRICES \n",
    "\n",
    "for i in range(num_features):\n",
    "    local_dim = s_features[i]\n",
    " \n",
    "    tmp = np.zeros((local_dim+1,rank))\n",
    "    tmp[0,:num_features]=1\n",
    "    tmp[0,i]=intercept/num_features   \n",
    "    tmp[1:s_features[i]+1,i]= coef[begin_feature[i]:begin_feature[i]+s_features[i]]\n",
    "\n",
    "    w_cores[i] = tmp.astype(np.float32) \n",
    "    \n",
    "# CORES\n",
    "    \n",
    "# list of tensor cores \n",
    "levels=math.ceil(np.log2(num_features)) # num levels, in this case 5 levels, excluding factor matrices (matrix on top included)\n",
    "num_tensors_last_level=num_features-2**(levels-1) # (26-2**4) 10 in this case\n",
    "\n",
    "w_tensors=[]\n",
    "for i in range(levels):\n",
    "    G_tensors_local=[]\n",
    "    for j in range(2**i):\n",
    "        if (i==levels-1) and (j==num_tensors_last_level):  # if in the last level and last tensor, break\n",
    "            break\n",
    "        G_tensors_local.append(None)\n",
    "    w_tensors.append(G_tensors_local)\n",
    "    \n",
    "identity_tensor=np.zeros(shape=(rank,rank,rank))\n",
    "for i in range(0,rank):\n",
    "    identity_tensor[i,i,i]=1\n",
    "       \n",
    "for i in range(len(w_tensors)):\n",
    "    for j in range(len(w_tensors[i])):\n",
    "        if i==0:\n",
    "            w_tensors[i][j]=np.eye(rank)\n",
    "        else:\n",
    "            w_tensors[i][j]=identity_tensor\n",
    "\n",
    "\n",
    "#%%\n",
    "# 0.001, 0.00001, 1.1, lr=1e-4\n",
    "            \n",
    "model = Tree_Machine(rank=rank, s_features=s_features, init_std=0.001, reg=5e-7, exp_reg=1.1) \n",
    "model.init_from_cores(w_cores) # factor matrix initialization\n",
    "model.init_from_cores_tensors(w_tensors) # tensor cores initialization\n",
    "model.build_graph()\n",
    "model.initialize_session()\n",
    "\n",
    "epoch_hist = []\n",
    "for epoch in range(21):\n",
    "    # train phase\n",
    "    loss_hist = []\n",
    "    penalty_hist = []\n",
    "    for x, y in simple_batcher(X_tr, y_tr, 256):\n",
    "        fd = {model.X: x, model.Y: 2*y-1}\n",
    "        run_ops = [model.trainer, model.outputs, model.loss, model.penalty]\n",
    "\n",
    "        _, outs, batch_loss, penalty = model.session.run(run_ops, fd)\n",
    "\n",
    "        loss_hist.append(batch_loss)\n",
    "        penalty_hist.append(penalty)\n",
    "        \n",
    "    epoch_train_loss = np.mean(loss_hist)\n",
    "    epoch_train_pen = np.mean(penalty_hist)\n",
    "    \n",
    "    epoch_stats = {\n",
    "        'epoch': epoch,\n",
    "        'train_logloss': float(epoch_train_loss)\n",
    "    }\n",
    "    \n",
    "    # test phase\n",
    "#    if epoch%2==0 and epoch>0:\n",
    "    fd = {model.X: X_te, model.Y: 2*y_te-1}\n",
    "    run_ops = [model.outputs, model.loss, model.penalty, model.penalized_loss]\n",
    "\n",
    "    outs, raw_loss, raw_penalty, loss = model.session.run(run_ops, fd)\n",
    "\n",
    "    epoch_test_loss = roc_auc_score(y_te, outs)\n",
    "    epoch_stats['test_auc'] = float(epoch_test_loss),\n",
    "#        epoch_stats['penalty'] = float(raw_penalty)\n",
    "    print('{}: te_auc: {:.4f}'.format(epoch, epoch_test_loss))\n",
    "epoch_hist.append(epoch_stats)\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
