{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CP_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wPu7OvU72y_",
        "colab_type": "code",
        "outputId": "7cc0e654-eb29-45bc-c3aa-023e96b26fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxCiIgZP8A6S",
        "colab_type": "code",
        "outputId": "a5d65a0a-2962-4c10-e457-80faf9feaf28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My\\ Drive/CPD/Regression\\ Tasks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CPD/Regression Tasks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRACDkmx8P-H",
        "colab_type": "code",
        "outputId": "cfe48432-f3e0-434c-f430-65eacd385ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from CP_Model import CP_Based\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout \n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnq_adCo8qYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def local_feature_mapping(x_train,x_val,x_test,local_dim=2,unit_norm=False):\n",
        "    \n",
        "    n_train_samples = x_train.shape[0]\n",
        "    n_val_samples = x_val.shape[0]\n",
        "    n_test_samples = x_test.shape[0]\n",
        "    n_features = x_train.shape[1]\n",
        "\n",
        "    X_transformed_train_list = []\n",
        "    X_transformed_val_list = []\n",
        "    X_transformed_test_list = []\n",
        "    \n",
        "    for d in range(local_dim-1):\n",
        "        X_transformed_train_list.append(x_train**(d+1))\n",
        "        X_transformed_val_list.append(x_val**(d+1))\n",
        "        X_transformed_test_list.append(x_test**(d+1))\n",
        "         \n",
        "    X_stand_train_list = [np.ones((n_train_samples,n_features))]\n",
        "    X_stand_val_list = [np.ones((n_val_samples,n_features))]\n",
        "    X_stand_test_list = [np.ones((n_test_samples,n_features))]\n",
        "    \n",
        "    for X_trans_train,X_trans_val,X_trans_test in zip(X_transformed_train_list,X_transformed_val_list,X_transformed_test_list):\n",
        "        \n",
        "        # scaler=StandardScaler()\n",
        "        # X_stand_train_list.append(scaler.fit_transform(X_trans_train))\n",
        "        # X_stand_val_list.append(scaler.transform(X_trans_val))    \n",
        "        # X_stand_test_list.append(scaler.transform(X_trans_test))  \n",
        "      X_stand_train_list.append(X_trans_train)\n",
        "      X_stand_val_list.append(X_trans_val) \n",
        "      X_stand_test_list.append(X_trans_test)\n",
        "        \n",
        "    x_train_processed= np.transpose(X_stand_train_list,(1, 2, 0))\n",
        "    x_val_processed= np.transpose(X_stand_val_list,(1, 2, 0))\n",
        "    x_test_processed = np.transpose(X_stand_test_list,(1, 2, 0))\n",
        "\n",
        "    if(unit_norm==True):\n",
        "      norms_train = np.sqrt(np.sum(x_train_processed**2,axis=-1,keepdims=True))\n",
        "      x_train_processed = np.divide(x_train_processed,norms_train)\n",
        "\n",
        "      norms_val = np.sqrt(np.sum(x_val_processed**2,axis=-1,keepdims=True))\n",
        "      x_val_processed = np.divide(x_val_processed,norms_val)\n",
        "    \n",
        "      norms_test = np.sqrt(np.sum(x_test_processed**2,axis=-1,keepdims=True))\n",
        "      x_test_processed = np.divide(x_test_processed,norms_test)\n",
        "\n",
        "    return x_train_processed, x_val_processed, x_test_processed      \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CI5E0a-fxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OrderReg(keras.regularizers.Regularizer):\n",
        "    def __init__(self,coef,beta,local_dim):\n",
        "        self.coef=coef\n",
        "        self.beta=beta\n",
        "        self.local_dim=local_dim\n",
        "    def __call__(self,kernel):\n",
        "        exp_weights = tf.constant([self.beta**i for i in range(self.local_dim)], shape=(self.local_dim, 1, 1, 1))\n",
        "        kernel_weighted = kernel*exp_weights\n",
        "        kernel_weighted = tf.transpose(kernel_weighted, perm=[2,3,0,1]) # N x units x d x m\n",
        "        penalty = tf.matmul(tf.transpose(kernel_weighted, perm=[0,1,3,2]),kernel_weighted)\n",
        "        return self.coef*tf.reduce_sum(penalty)\n",
        "    def get_config(self):\n",
        "        return{\"coef\":self.coef,\"beta\":self.beta,\"local_dim\":self.local_dim}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xEBCXp09exz",
        "colab_type": "text"
      },
      "source": [
        "#Artificial Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iotDHcf99hBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "N_samples = 3000\n",
        "N_features = 7\n",
        "std_noise = 100.0\n",
        "\n",
        "X = np.random.uniform(0,10,(N_samples,N_features))\n",
        "#y = -5+3*X[:,0]-2*X[:,1]-2*X[:,2]+4*X[:,3]+2*X[:,0]*X[:,1]+1.0*X[:,1]*X[:,2]-0.8*X[:,2]*X[:,3]+0.5*X[:,0]*X[:,1]*X[:,3]+np.random.normal(0.0,std_noise,N_samples)\n",
        "\n",
        "y = 1000+6*X[:,0]-4*X[:,2]-3*X[:,0]*X[:,1]+4*X[:,2]*X[:,3]+3*X[:,0]*X[:,1]+0.5*(X[:,0]**2)*X[:,1]+0.2*(X[:,1]**2)*X[:,2]+0.1*(X[:,1]**2)*X[:,2]*X[:,3]+np.random.normal(0.0,std_noise,N_samples)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train, X_val, X_test= scaler_X.fit_transform(X_train), scaler_X.transform(X_val), scaler_X.transform(X_test)\n",
        "y_train, y_val = scaler_y.fit_transform(y_train.reshape(-1,1)).ravel(), scaler_y.transform(y_val.reshape(-1,1)).ravel()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mAhuv2AsLXb",
        "colab_type": "code",
        "outputId": "36ea2d6d-c4c2-4d23-c9a1-84a280e95ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "##### Linear Regression #####\n",
        "start = time.time()\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_train)\n",
        "y_pred = lin_reg.predict(X_val)\n",
        "val_mse = mean_squared_error(y_pred, y_val)\n",
        "#cv_score = cross_val_score(lin_reg,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "#print('Linear Regression MSE:{}'.format(-cv_score.mean()))\n",
        "print('Linear Regression MSE:{}'.format(val_mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.004152059555053711\n",
            "Linear Regression MSE:0.33115400605449596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEf8n_vPsf2A",
        "colab_type": "code",
        "outputId": "35c7adfe-20f1-4ff4-c368-b7af0fd64699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#### SVM RBF Kernel ####\n",
        "start = time.time()\n",
        "svr_rbf = SVR(kernel='rbf',C=10.0)\n",
        "svr_rbf.fit(X_train,y_train)\n",
        "y_pred = svr_rbf.predict(X_val)\n",
        "val_mse = mean_squared_error(y_pred, y_val)\n",
        "#cv_score = cross_val_score(svr_rbf,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "#print('SVR RBF MSE:{}'.format(-cv_score.mean()))\n",
        "print('SVR RBF MSE:{}'.format(val_mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2285923957824707\n",
            "SVR RBF MSE:0.1790704982678583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eETTujDBswqd",
        "colab_type": "code",
        "outputId": "70d600d8-e047-4d56-f368-6805f45276e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#### Neural Network ####\n",
        "model_nn=Sequential()\n",
        "model_nn.add(Dense(20,activation='relu'))\n",
        "model_nn.add(BatchNormalization())\n",
        "model_nn.add(Dense(20,activation='relu'))\n",
        "model_nn.add(BatchNormalization())\n",
        "model_nn.add(Dense(15,activation='relu'))\n",
        "model_nn.add(BatchNormalization())\n",
        "model_nn.add(Dense(1,activation=None))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model_nn.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "start = time.time()\n",
        "history_nn=model_nn.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_val,y_val))\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "print('NN MSE:{}'.format(min(history_nn.history['val_loss'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1920 samples, validate on 480 samples\n",
            "Epoch 1/100\n",
            "1920/1920 [==============================] - 2s 916us/sample - loss: 1.0884 - val_loss: 0.7653\n",
            "Epoch 2/100\n",
            "1920/1920 [==============================] - 0s 89us/sample - loss: 0.6057 - val_loss: 0.5549\n",
            "Epoch 3/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.4578 - val_loss: 0.3982\n",
            "Epoch 4/100\n",
            "1920/1920 [==============================] - 0s 87us/sample - loss: 0.3811 - val_loss: 0.2949\n",
            "Epoch 5/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.3306 - val_loss: 0.2628\n",
            "Epoch 6/100\n",
            "1920/1920 [==============================] - 0s 101us/sample - loss: 0.2915 - val_loss: 0.2583\n",
            "Epoch 7/100\n",
            "1920/1920 [==============================] - 0s 98us/sample - loss: 0.2969 - val_loss: 0.2415\n",
            "Epoch 8/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.2756 - val_loss: 0.2288\n",
            "Epoch 9/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.2644 - val_loss: 0.2211\n",
            "Epoch 10/100\n",
            "1920/1920 [==============================] - 0s 95us/sample - loss: 0.2485 - val_loss: 0.2193\n",
            "Epoch 11/100\n",
            "1920/1920 [==============================] - 0s 87us/sample - loss: 0.2260 - val_loss: 0.2124\n",
            "Epoch 12/100\n",
            "1920/1920 [==============================] - 0s 83us/sample - loss: 0.2514 - val_loss: 0.2071\n",
            "Epoch 13/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.2486 - val_loss: 0.1985\n",
            "Epoch 14/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.2183 - val_loss: 0.1976\n",
            "Epoch 15/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.2251 - val_loss: 0.1954\n",
            "Epoch 16/100\n",
            "1920/1920 [==============================] - 0s 98us/sample - loss: 0.2174 - val_loss: 0.1985\n",
            "Epoch 17/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.2213 - val_loss: 0.1951\n",
            "Epoch 18/100\n",
            "1920/1920 [==============================] - 0s 103us/sample - loss: 0.2259 - val_loss: 0.1931\n",
            "Epoch 19/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.2193 - val_loss: 0.1943\n",
            "Epoch 20/100\n",
            "1920/1920 [==============================] - 0s 89us/sample - loss: 0.2247 - val_loss: 0.1846\n",
            "Epoch 21/100\n",
            "1920/1920 [==============================] - 0s 83us/sample - loss: 0.2057 - val_loss: 0.1815\n",
            "Epoch 22/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.2083 - val_loss: 0.1845\n",
            "Epoch 23/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.2056 - val_loss: 0.1773\n",
            "Epoch 24/100\n",
            "1920/1920 [==============================] - 0s 101us/sample - loss: 0.2158 - val_loss: 0.1774\n",
            "Epoch 25/100\n",
            "1920/1920 [==============================] - 0s 100us/sample - loss: 0.2242 - val_loss: 0.1763\n",
            "Epoch 26/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.2039 - val_loss: 0.1744\n",
            "Epoch 27/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1960 - val_loss: 0.1713\n",
            "Epoch 28/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.2063 - val_loss: 0.1684\n",
            "Epoch 29/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1964 - val_loss: 0.1707\n",
            "Epoch 30/100\n",
            "1920/1920 [==============================] - 0s 100us/sample - loss: 0.1928 - val_loss: 0.1694\n",
            "Epoch 31/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.2070 - val_loss: 0.1708\n",
            "Epoch 32/100\n",
            "1920/1920 [==============================] - 0s 87us/sample - loss: 0.2021 - val_loss: 0.1731\n",
            "Epoch 33/100\n",
            "1920/1920 [==============================] - 0s 85us/sample - loss: 0.2053 - val_loss: 0.1650\n",
            "Epoch 34/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1920 - val_loss: 0.1692\n",
            "Epoch 35/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.1918 - val_loss: 0.1645\n",
            "Epoch 36/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1906 - val_loss: 0.1633\n",
            "Epoch 37/100\n",
            "1920/1920 [==============================] - 0s 93us/sample - loss: 0.1900 - val_loss: 0.1642\n",
            "Epoch 38/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1903 - val_loss: 0.1633\n",
            "Epoch 39/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1917 - val_loss: 0.1598\n",
            "Epoch 40/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.2034 - val_loss: 0.1604\n",
            "Epoch 41/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.1697 - val_loss: 0.1640\n",
            "Epoch 42/100\n",
            "1920/1920 [==============================] - 0s 105us/sample - loss: 0.1811 - val_loss: 0.1638\n",
            "Epoch 43/100\n",
            "1920/1920 [==============================] - 0s 94us/sample - loss: 0.1871 - val_loss: 0.1648\n",
            "Epoch 44/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1802 - val_loss: 0.1613\n",
            "Epoch 45/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1836 - val_loss: 0.1612\n",
            "Epoch 46/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1812 - val_loss: 0.1585\n",
            "Epoch 47/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1954 - val_loss: 0.1610\n",
            "Epoch 48/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1862 - val_loss: 0.1598\n",
            "Epoch 49/100\n",
            "1920/1920 [==============================] - 0s 89us/sample - loss: 0.1825 - val_loss: 0.1644\n",
            "Epoch 50/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.1899 - val_loss: 0.1637\n",
            "Epoch 51/100\n",
            "1920/1920 [==============================] - 0s 89us/sample - loss: 0.1816 - val_loss: 0.1610\n",
            "Epoch 52/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.1844 - val_loss: 0.1605\n",
            "Epoch 53/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1927 - val_loss: 0.1587\n",
            "Epoch 54/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.1816 - val_loss: 0.1591\n",
            "Epoch 55/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1810 - val_loss: 0.1637\n",
            "Epoch 56/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1797 - val_loss: 0.1611\n",
            "Epoch 57/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1742 - val_loss: 0.1599\n",
            "Epoch 58/100\n",
            "1920/1920 [==============================] - 0s 89us/sample - loss: 0.1853 - val_loss: 0.1597\n",
            "Epoch 59/100\n",
            "1920/1920 [==============================] - 0s 93us/sample - loss: 0.1885 - val_loss: 0.1660\n",
            "Epoch 60/100\n",
            "1920/1920 [==============================] - 0s 101us/sample - loss: 0.1758 - val_loss: 0.1621\n",
            "Epoch 61/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1651 - val_loss: 0.1600\n",
            "Epoch 62/100\n",
            "1920/1920 [==============================] - 0s 95us/sample - loss: 0.1755 - val_loss: 0.1578\n",
            "Epoch 63/100\n",
            "1920/1920 [==============================] - 0s 98us/sample - loss: 0.1839 - val_loss: 0.1578\n",
            "Epoch 64/100\n",
            "1920/1920 [==============================] - 0s 101us/sample - loss: 0.1763 - val_loss: 0.1574\n",
            "Epoch 65/100\n",
            "1920/1920 [==============================] - 0s 94us/sample - loss: 0.1790 - val_loss: 0.1577\n",
            "Epoch 66/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1829 - val_loss: 0.1578\n",
            "Epoch 67/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1814 - val_loss: 0.1610\n",
            "Epoch 68/100\n",
            "1920/1920 [==============================] - 0s 107us/sample - loss: 0.1741 - val_loss: 0.1575\n",
            "Epoch 69/100\n",
            "1920/1920 [==============================] - 0s 109us/sample - loss: 0.1675 - val_loss: 0.1583\n",
            "Epoch 70/100\n",
            "1920/1920 [==============================] - 0s 101us/sample - loss: 0.1769 - val_loss: 0.1582\n",
            "Epoch 71/100\n",
            "1920/1920 [==============================] - 0s 96us/sample - loss: 0.1726 - val_loss: 0.1642\n",
            "Epoch 72/100\n",
            "1920/1920 [==============================] - 0s 99us/sample - loss: 0.1599 - val_loss: 0.1569\n",
            "Epoch 73/100\n",
            "1920/1920 [==============================] - 0s 103us/sample - loss: 0.1789 - val_loss: 0.1553\n",
            "Epoch 74/100\n",
            "1920/1920 [==============================] - 0s 103us/sample - loss: 0.1711 - val_loss: 0.1594\n",
            "Epoch 75/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1812 - val_loss: 0.1555\n",
            "Epoch 76/100\n",
            "1920/1920 [==============================] - 0s 96us/sample - loss: 0.1816 - val_loss: 0.1602\n",
            "Epoch 77/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1787 - val_loss: 0.1605\n",
            "Epoch 78/100\n",
            "1920/1920 [==============================] - 0s 108us/sample - loss: 0.1816 - val_loss: 0.1581\n",
            "Epoch 79/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1681 - val_loss: 0.1623\n",
            "Epoch 80/100\n",
            "1920/1920 [==============================] - 0s 97us/sample - loss: 0.1890 - val_loss: 0.1574\n",
            "Epoch 81/100\n",
            "1920/1920 [==============================] - 0s 88us/sample - loss: 0.1686 - val_loss: 0.1617\n",
            "Epoch 82/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1859 - val_loss: 0.1603\n",
            "Epoch 83/100\n",
            "1920/1920 [==============================] - 0s 98us/sample - loss: 0.1622 - val_loss: 0.1559\n",
            "Epoch 84/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1608 - val_loss: 0.1567\n",
            "Epoch 85/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.1594 - val_loss: 0.1638\n",
            "Epoch 86/100\n",
            "1920/1920 [==============================] - 0s 91us/sample - loss: 0.1825 - val_loss: 0.1615\n",
            "Epoch 87/100\n",
            "1920/1920 [==============================] - 0s 103us/sample - loss: 0.1706 - val_loss: 0.1617\n",
            "Epoch 88/100\n",
            "1920/1920 [==============================] - 0s 99us/sample - loss: 0.1597 - val_loss: 0.1609\n",
            "Epoch 89/100\n",
            "1920/1920 [==============================] - 0s 98us/sample - loss: 0.1590 - val_loss: 0.1597\n",
            "Epoch 90/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1692 - val_loss: 0.1596\n",
            "Epoch 91/100\n",
            "1920/1920 [==============================] - 0s 102us/sample - loss: 0.1816 - val_loss: 0.1546\n",
            "Epoch 92/100\n",
            "1920/1920 [==============================] - 0s 94us/sample - loss: 0.1861 - val_loss: 0.1578\n",
            "Epoch 93/100\n",
            "1920/1920 [==============================] - 0s 100us/sample - loss: 0.1725 - val_loss: 0.1592\n",
            "Epoch 94/100\n",
            "1920/1920 [==============================] - 0s 105us/sample - loss: 0.1715 - val_loss: 0.1563\n",
            "Epoch 95/100\n",
            "1920/1920 [==============================] - 0s 106us/sample - loss: 0.1589 - val_loss: 0.1591\n",
            "Epoch 96/100\n",
            "1920/1920 [==============================] - 0s 103us/sample - loss: 0.1699 - val_loss: 0.1594\n",
            "Epoch 97/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1820 - val_loss: 0.1600\n",
            "Epoch 98/100\n",
            "1920/1920 [==============================] - 0s 92us/sample - loss: 0.1610 - val_loss: 0.1605\n",
            "Epoch 99/100\n",
            "1920/1920 [==============================] - 0s 90us/sample - loss: 0.1695 - val_loss: 0.1578\n",
            "Epoch 100/100\n",
            "1920/1920 [==============================] - 0s 100us/sample - loss: 0.1711 - val_loss: 0.1605\n",
            "20.590782403945923\n",
            "NN MSE:0.15457460383574168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEKNNJYt9vZd",
        "colab_type": "code",
        "outputId": "6f40c9af-96fa-4620-8c38-9fbeb9c60c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cp_rank=10\n",
        "local_dim=3\n",
        "x_train_processed,x_val_processed,x_test_processed=local_feature_mapping(X_train,X_val,X_test,local_dim=local_dim)\n",
        "print('rank: {}, local dimension:{}'.format(cp_rank,local_dim))\n",
        "\n",
        "model=Sequential()\n",
        "model.add(CP_Based(units=1,activation=None,cp_rank=cp_rank,local_dim=local_dim,\n",
        "                    initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.2)))\n",
        "    # regularizer=OrderReg(coef=1e-3,beta=1.0,local_dim=local_dim),\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "start = time.time()\n",
        "history=model.fit(x_train_processed, y_train, epochs=100, batch_size=32,validation_data=(x_val_processed,y_val))\n",
        "end = time.time()\n",
        "\n",
        "train_loss=min(history.history['loss'])\n",
        "val_loss=min(history.history['val_loss'])\n",
        "print('Training Time:{}'.format(end-start))\n",
        "print('train loss:{}'.format(train_loss))\n",
        "print('validation loss:{}'.format(val_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rank: 10, local dimension:3\n",
            "WARNING:tensorflow:Entity <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f9114248438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f9114248438>>, which Python reported as:\n",
            "    def call(self, X):\n",
            "            \n",
            "#        feat_tensor = tf.transpose([1-X,X],perm=[1, 2, 0]) # for images \n",
            "#        feat_tensor = tf.transpose([tf.sqrt(1-X**2),X],perm=[1, 2, 0]) # fpr regression\n",
            "#        X_square=X**2\n",
            "\n",
            "#        X_square=(X_square-tf.reduce_mean(X_square,axis=0,keepdims=True))/tf.math.reduce_std(X_square,axis=0,keepdims=True)\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X,X_square],perm=[1, 2, 0]) # fpr regression\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X],perm=[1, 2, 0]) # fpr regression\n",
            "        feat_tensor=X\n",
            "        \n",
            "        output_list=[]\n",
            "        \n",
            "        for unit in range(0,self.units):                 \n",
            "                    \n",
            "            feat_tensor_reshaped=tf.transpose(feat_tensor,perm=[1,0,2]) # NxNtxd\n",
            "            weights=tf.transpose(self.kernel[:,:,:,unit],perm=[2,0,1]) # Nxdxm\n",
            "            test=tf.matmul(feat_tensor_reshaped,weights) # NxNtxm\n",
            "            test_hadamard=tf.reduce_prod(test,axis=0) # Ntxm   \n",
            "            output=tf.reduce_sum(test_hadamard, axis=1) # Ntx1\n",
            "            output_list.append(output)\n",
            "        \n",
            "        to_return=tf.stack(output_list, axis=1)\n",
            "        return self.activation(to_return)\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "WARNING: Entity <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f9114248438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f9114248438>>, which Python reported as:\n",
            "    def call(self, X):\n",
            "            \n",
            "#        feat_tensor = tf.transpose([1-X,X],perm=[1, 2, 0]) # for images \n",
            "#        feat_tensor = tf.transpose([tf.sqrt(1-X**2),X],perm=[1, 2, 0]) # fpr regression\n",
            "#        X_square=X**2\n",
            "\n",
            "#        X_square=(X_square-tf.reduce_mean(X_square,axis=0,keepdims=True))/tf.math.reduce_std(X_square,axis=0,keepdims=True)\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X,X_square],perm=[1, 2, 0]) # fpr regression\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X],perm=[1, 2, 0]) # fpr regression\n",
            "        feat_tensor=X\n",
            "        \n",
            "        output_list=[]\n",
            "        \n",
            "        for unit in range(0,self.units):                 \n",
            "                    \n",
            "            feat_tensor_reshaped=tf.transpose(feat_tensor,perm=[1,0,2]) # NxNtxd\n",
            "            weights=tf.transpose(self.kernel[:,:,:,unit],perm=[2,0,1]) # Nxdxm\n",
            "            test=tf.matmul(feat_tensor_reshaped,weights) # NxNtxm\n",
            "            test_hadamard=tf.reduce_prod(test,axis=0) # Ntxm   \n",
            "            output=tf.reduce_sum(test_hadamard, axis=1) # Ntx1\n",
            "            output_list.append(output)\n",
            "        \n",
            "        to_return=tf.stack(output_list, axis=1)\n",
            "        return self.activation(to_return)\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "Train on 1920 samples, validate on 480 samples\n",
            "Epoch 1/100\n",
            "1920/1920 [==============================] - 1s 290us/sample - loss: 1.0000 - val_loss: 1.0364\n",
            "Epoch 2/100\n",
            "1920/1920 [==============================] - 0s 46us/sample - loss: 0.9998 - val_loss: 1.0360\n",
            "Epoch 3/100\n",
            "1920/1920 [==============================] - 0s 48us/sample - loss: 0.9951 - val_loss: 1.0201\n",
            "Epoch 4/100\n",
            "1920/1920 [==============================] - 0s 47us/sample - loss: 0.9090 - val_loss: 0.8707\n",
            "Epoch 5/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.7603 - val_loss: 0.7681\n",
            "Epoch 6/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.6688 - val_loss: 0.7422\n",
            "Epoch 7/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.5925 - val_loss: 0.6170\n",
            "Epoch 8/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.5445 - val_loss: 0.5806\n",
            "Epoch 9/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.5099 - val_loss: 0.5381\n",
            "Epoch 10/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.4870 - val_loss: 0.5117\n",
            "Epoch 11/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.4636 - val_loss: 0.4958\n",
            "Epoch 12/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.4418 - val_loss: 0.4656\n",
            "Epoch 13/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.4239 - val_loss: 0.4463\n",
            "Epoch 14/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.4016 - val_loss: 0.4323\n",
            "Epoch 15/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.3828 - val_loss: 0.4001\n",
            "Epoch 16/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.3628 - val_loss: 0.3863\n",
            "Epoch 17/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.3445 - val_loss: 0.3704\n",
            "Epoch 18/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.3290 - val_loss: 0.3599\n",
            "Epoch 19/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.3110 - val_loss: 0.3339\n",
            "Epoch 20/100\n",
            "1920/1920 [==============================] - 0s 48us/sample - loss: 0.2965 - val_loss: 0.3185\n",
            "Epoch 21/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.2824 - val_loss: 0.3037\n",
            "Epoch 22/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.2706 - val_loss: 0.2866\n",
            "Epoch 23/100\n",
            "1920/1920 [==============================] - 0s 57us/sample - loss: 0.2574 - val_loss: 0.2740\n",
            "Epoch 24/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.2473 - val_loss: 0.2587\n",
            "Epoch 25/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.2363 - val_loss: 0.2480\n",
            "Epoch 26/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.2247 - val_loss: 0.2371\n",
            "Epoch 27/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.2155 - val_loss: 0.2233\n",
            "Epoch 28/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.2073 - val_loss: 0.2153\n",
            "Epoch 29/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1978 - val_loss: 0.2049\n",
            "Epoch 30/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1908 - val_loss: 0.1978\n",
            "Epoch 31/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1840 - val_loss: 0.1888\n",
            "Epoch 32/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1772 - val_loss: 0.1832\n",
            "Epoch 33/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1715 - val_loss: 0.1786\n",
            "Epoch 34/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1675 - val_loss: 0.1746\n",
            "Epoch 35/100\n",
            "1920/1920 [==============================] - 0s 56us/sample - loss: 0.1626 - val_loss: 0.1695\n",
            "Epoch 36/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1584 - val_loss: 0.1681\n",
            "Epoch 37/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1554 - val_loss: 0.1628\n",
            "Epoch 38/100\n",
            "1920/1920 [==============================] - 0s 56us/sample - loss: 0.1523 - val_loss: 0.1617\n",
            "Epoch 39/100\n",
            "1920/1920 [==============================] - 0s 56us/sample - loss: 0.1495 - val_loss: 0.1595\n",
            "Epoch 40/100\n",
            "1920/1920 [==============================] - 0s 59us/sample - loss: 0.1471 - val_loss: 0.1575\n",
            "Epoch 41/100\n",
            "1920/1920 [==============================] - 0s 66us/sample - loss: 0.1458 - val_loss: 0.1557\n",
            "Epoch 42/100\n",
            "1920/1920 [==============================] - 0s 62us/sample - loss: 0.1434 - val_loss: 0.1535\n",
            "Epoch 43/100\n",
            "1920/1920 [==============================] - 0s 56us/sample - loss: 0.1418 - val_loss: 0.1516\n",
            "Epoch 44/100\n",
            "1920/1920 [==============================] - 0s 57us/sample - loss: 0.1401 - val_loss: 0.1511\n",
            "Epoch 45/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.1388 - val_loss: 0.1509\n",
            "Epoch 46/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1381 - val_loss: 0.1488\n",
            "Epoch 47/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1371 - val_loss: 0.1477\n",
            "Epoch 48/100\n",
            "1920/1920 [==============================] - 0s 56us/sample - loss: 0.1357 - val_loss: 0.1471\n",
            "Epoch 49/100\n",
            "1920/1920 [==============================] - 0s 62us/sample - loss: 0.1348 - val_loss: 0.1463\n",
            "Epoch 50/100\n",
            "1920/1920 [==============================] - 0s 65us/sample - loss: 0.1342 - val_loss: 0.1463\n",
            "Epoch 51/100\n",
            "1920/1920 [==============================] - 0s 61us/sample - loss: 0.1339 - val_loss: 0.1451\n",
            "Epoch 52/100\n",
            "1920/1920 [==============================] - 0s 61us/sample - loss: 0.1325 - val_loss: 0.1472\n",
            "Epoch 53/100\n",
            "1920/1920 [==============================] - 0s 59us/sample - loss: 0.1332 - val_loss: 0.1432\n",
            "Epoch 54/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1314 - val_loss: 0.1426\n",
            "Epoch 55/100\n",
            "1920/1920 [==============================] - 0s 58us/sample - loss: 0.1311 - val_loss: 0.1435\n",
            "Epoch 56/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1314 - val_loss: 0.1420\n",
            "Epoch 57/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1297 - val_loss: 0.1423\n",
            "Epoch 58/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1296 - val_loss: 0.1410\n",
            "Epoch 59/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1290 - val_loss: 0.1414\n",
            "Epoch 60/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1292 - val_loss: 0.1405\n",
            "Epoch 61/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1285 - val_loss: 0.1401\n",
            "Epoch 62/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1283 - val_loss: 0.1402\n",
            "Epoch 63/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1284 - val_loss: 0.1399\n",
            "Epoch 64/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1280 - val_loss: 0.1399\n",
            "Epoch 65/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.1275 - val_loss: 0.1393\n",
            "Epoch 66/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.1279 - val_loss: 0.1393\n",
            "Epoch 67/100\n",
            "1920/1920 [==============================] - 0s 63us/sample - loss: 0.1269 - val_loss: 0.1412\n",
            "Epoch 68/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1270 - val_loss: 0.1387\n",
            "Epoch 69/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1270 - val_loss: 0.1398\n",
            "Epoch 70/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1263 - val_loss: 0.1391\n",
            "Epoch 71/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.1264 - val_loss: 0.1393\n",
            "Epoch 72/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1262 - val_loss: 0.1387\n",
            "Epoch 73/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.1261 - val_loss: 0.1391\n",
            "Epoch 74/100\n",
            "1920/1920 [==============================] - 0s 57us/sample - loss: 0.1254 - val_loss: 0.1398\n",
            "Epoch 75/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1259 - val_loss: 0.1397\n",
            "Epoch 76/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1259 - val_loss: 0.1384\n",
            "Epoch 77/100\n",
            "1920/1920 [==============================] - 0s 53us/sample - loss: 0.1258 - val_loss: 0.1384\n",
            "Epoch 78/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1258 - val_loss: 0.1383\n",
            "Epoch 79/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1259 - val_loss: 0.1387\n",
            "Epoch 80/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1255 - val_loss: 0.1384\n",
            "Epoch 81/100\n",
            "1920/1920 [==============================] - 0s 58us/sample - loss: 0.1250 - val_loss: 0.1390\n",
            "Epoch 82/100\n",
            "1920/1920 [==============================] - 0s 55us/sample - loss: 0.1248 - val_loss: 0.1386\n",
            "Epoch 83/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.1251 - val_loss: 0.1385\n",
            "Epoch 84/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1253 - val_loss: 0.1378\n",
            "Epoch 85/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1247 - val_loss: 0.1387\n",
            "Epoch 86/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.1246 - val_loss: 0.1384\n",
            "Epoch 87/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1245 - val_loss: 0.1388\n",
            "Epoch 88/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.1247 - val_loss: 0.1389\n",
            "Epoch 89/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1244 - val_loss: 0.1390\n",
            "Epoch 90/100\n",
            "1920/1920 [==============================] - 0s 59us/sample - loss: 0.1247 - val_loss: 0.1389\n",
            "Epoch 91/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1243 - val_loss: 0.1387\n",
            "Epoch 92/100\n",
            "1920/1920 [==============================] - 0s 52us/sample - loss: 0.1249 - val_loss: 0.1384\n",
            "Epoch 93/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1246 - val_loss: 0.1385\n",
            "Epoch 94/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1239 - val_loss: 0.1408\n",
            "Epoch 95/100\n",
            "1920/1920 [==============================] - 0s 50us/sample - loss: 0.1244 - val_loss: 0.1391\n",
            "Epoch 96/100\n",
            "1920/1920 [==============================] - 0s 54us/sample - loss: 0.1246 - val_loss: 0.1389\n",
            "Epoch 97/100\n",
            "1920/1920 [==============================] - 0s 49us/sample - loss: 0.1238 - val_loss: 0.1395\n",
            "Epoch 98/100\n",
            "1920/1920 [==============================] - 0s 51us/sample - loss: 0.1243 - val_loss: 0.1385\n",
            "Epoch 99/100\n",
            "1920/1920 [==============================] - 0s 48us/sample - loss: 0.1238 - val_loss: 0.1389\n",
            "Epoch 100/100\n",
            "1920/1920 [==============================] - 0s 47us/sample - loss: 0.1240 - val_loss: 0.1394\n",
            "Training Time:11.097119092941284\n",
            "train loss:0.12375357622901599\n",
            "validation loss:0.13782398303349813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-gGhgPr9cuz",
        "colab_type": "text"
      },
      "source": [
        "#California Housing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cucKsRG9cMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Data preprocessing ###\n",
        "dataset = fetch_california_housing()\n",
        "X_full, Y_full = dataset.data, dataset.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_full, Y_full, test_size = 0.2, random_state = 0)\n",
        "x_train, x_val, y_train, y_val = x_train[:13208],x_train[13208:], y_train[:13208],y_train[13208:]\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_Y = StandardScaler()\n",
        "\n",
        "x_train, x_val, x_test= scaler_X.fit_transform(x_train), scaler_X.transform(x_val), scaler_X.transform(x_test)\n",
        "y_train, y_val, y_test = scaler_Y.fit_transform(y_train.reshape(-1,1)).ravel(), scaler_Y.transform(y_val.reshape(-1,1)).ravel(), scaler_Y.transform(y_test.reshape(-1,1)).ravel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SXndWQUxR-D",
        "colab_type": "code",
        "outputId": "76f4d433-456e-47cc-d059-56c5013b99e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "##### Linear Regression #####\n",
        "start = time.time()\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x_train,y_train)\n",
        "y_pred = lin_reg.predict(x_val)\n",
        "val_mse = mean_squared_error(y_pred, y_val)\n",
        "#cv_score = cross_val_score(lin_reg,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "#print('Linear Regression MSE:{}'.format(-cv_score.mean()))\n",
        "print('Linear Regression MSE:{}'.format(val_mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.023911476135253906\n",
            "Linear Regression MSE:0.3712142442969637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAL1QkZ6xiXh",
        "colab_type": "code",
        "outputId": "21962e6b-c14d-4da9-8fcd-78932980b7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#### SVR Polynomial Kernel ####\n",
        "start = time.time()\n",
        "svr_poly = SVR(kernel='poly',degree=3,C=1.0,epsilon=1)\n",
        "svr_poly.fit(x_train,y_train)\n",
        "y_pred = svr_poly.predict(x_val)\n",
        "y_pred_train = svr_poly.predict(x_train)\n",
        "train_mse = mean_squared_error(y_pred_train, y_train)\n",
        "val_mse = mean_squared_error(y_pred, y_val)\n",
        "#cv_score = cross_val_score(svr_poly,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "#print('SVR Poly MSE:{}'.format(-cv_score.mean()))\n",
        "print('SVR Train Poly MSE:{}'.format(train_mse))\n",
        "print('SVR Poly MSE:{}'.format(val_mse))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.855961322784424\n",
            "SVR Train Poly MSE:0.5222889568347701\n",
            "SVR Poly MSE:0.5979396648103871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-dH6IdXxuBp",
        "colab_type": "code",
        "outputId": "6984dfde-d583-461d-c8b0-14da7867d8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#### SVR RBF Kernel ####\n",
        "start = time.time()\n",
        "svr_rbf = SVR(kernel='rbf',C=10.0)\n",
        "svr_rbf.fit(x_train,y_train)\n",
        "y_pred = svr_rbf.predict(x_val)\n",
        "val_mse = mean_squared_error(y_pred, y_val)\n",
        "#cv_score = cross_val_score(svr_rbf,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "#print('SVR RBF MSE:{}'.format(-cv_score.mean()))\n",
        "print('SVR RBF MSE:{}'.format(val_mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.78395414352417\n",
            "SVR RBF MSE:0.22361283511419183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4XPnQFj68xY",
        "colab_type": "code",
        "outputId": "064df656-9a9b-4f18-b18e-659a905387f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#### Neural Network ####\n",
        "model=Sequential()\n",
        "model.add(Dense(40,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "start = time.time()\n",
        "history=model.fit(x_train, y_train, epochs=100, batch_size=32,validation_data=(x_val,y_val))\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "print('NN MSE:{}'.format(min(history.history['val_loss'])))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13208 samples, validate on 3304 samples\n",
            "Epoch 1/100\n",
            "13208/13208 [==============================] - 3s 242us/sample - loss: 0.5086 - val_loss: 0.3450\n",
            "Epoch 2/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.3571 - val_loss: 0.3050\n",
            "Epoch 3/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.3251 - val_loss: 0.2691\n",
            "Epoch 4/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.3088 - val_loss: 0.2677\n",
            "Epoch 5/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2959 - val_loss: 0.2447\n",
            "Epoch 6/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2884 - val_loss: 0.2616\n",
            "Epoch 7/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2818 - val_loss: 0.2530\n",
            "Epoch 8/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2755 - val_loss: 0.2673\n",
            "Epoch 9/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2709 - val_loss: 0.2518\n",
            "Epoch 10/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2682 - val_loss: 0.2347\n",
            "Epoch 11/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2644 - val_loss: 0.2231\n",
            "Epoch 12/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2646 - val_loss: 0.2644\n",
            "Epoch 13/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2619 - val_loss: 0.2353\n",
            "Epoch 14/100\n",
            "13208/13208 [==============================] - 2s 120us/sample - loss: 0.2622 - val_loss: 0.2647\n",
            "Epoch 15/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2563 - val_loss: 0.2365\n",
            "Epoch 16/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2549 - val_loss: 0.2247\n",
            "Epoch 17/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2526 - val_loss: 0.2413\n",
            "Epoch 18/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2536 - val_loss: 0.2387\n",
            "Epoch 19/100\n",
            "13208/13208 [==============================] - 2s 120us/sample - loss: 0.2487 - val_loss: 0.2280\n",
            "Epoch 20/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2457 - val_loss: 0.2479\n",
            "Epoch 21/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2486 - val_loss: 0.2200\n",
            "Epoch 22/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2427 - val_loss: 0.2108\n",
            "Epoch 23/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2445 - val_loss: 0.2333\n",
            "Epoch 24/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2457 - val_loss: 0.2123\n",
            "Epoch 25/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2412 - val_loss: 0.2174\n",
            "Epoch 26/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2446 - val_loss: 0.2387\n",
            "Epoch 27/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2334 - val_loss: 0.2325\n",
            "Epoch 28/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2429 - val_loss: 0.2221\n",
            "Epoch 29/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2331 - val_loss: 0.2444\n",
            "Epoch 30/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2363 - val_loss: 0.2085\n",
            "Epoch 31/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2347 - val_loss: 0.2244\n",
            "Epoch 32/100\n",
            "13208/13208 [==============================] - 2s 122us/sample - loss: 0.2397 - val_loss: 0.2126\n",
            "Epoch 33/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2327 - val_loss: 0.2175\n",
            "Epoch 34/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2394 - val_loss: 0.2264\n",
            "Epoch 35/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2344 - val_loss: 0.2210\n",
            "Epoch 36/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2302 - val_loss: 0.2205\n",
            "Epoch 37/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2311 - val_loss: 0.2156\n",
            "Epoch 38/100\n",
            "13208/13208 [==============================] - 2s 120us/sample - loss: 0.2345 - val_loss: 0.2334\n",
            "Epoch 39/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2316 - val_loss: 0.2169\n",
            "Epoch 40/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2266 - val_loss: 0.2175\n",
            "Epoch 41/100\n",
            "13208/13208 [==============================] - 2s 119us/sample - loss: 0.2285 - val_loss: 0.2460\n",
            "Epoch 42/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2279 - val_loss: 0.2265\n",
            "Epoch 43/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2264 - val_loss: 0.2198\n",
            "Epoch 44/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2264 - val_loss: 0.2260\n",
            "Epoch 45/100\n",
            "13208/13208 [==============================] - 2s 118us/sample - loss: 0.2237 - val_loss: 0.2166\n",
            "Epoch 46/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2209 - val_loss: 0.2208\n",
            "Epoch 47/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2252 - val_loss: 0.2065\n",
            "Epoch 48/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2253 - val_loss: 0.2253\n",
            "Epoch 49/100\n",
            "13208/13208 [==============================] - 2s 118us/sample - loss: 0.2215 - val_loss: 0.2128\n",
            "Epoch 50/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2250 - val_loss: 0.2095\n",
            "Epoch 51/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2193 - val_loss: 0.2320\n",
            "Epoch 52/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2220 - val_loss: 0.2246\n",
            "Epoch 53/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2185 - val_loss: 0.2236\n",
            "Epoch 54/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2293 - val_loss: 0.2331\n",
            "Epoch 55/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2242 - val_loss: 0.2224\n",
            "Epoch 56/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2202 - val_loss: 0.2168\n",
            "Epoch 57/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2190 - val_loss: 0.2111\n",
            "Epoch 58/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2173 - val_loss: 0.2061\n",
            "Epoch 59/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2186 - val_loss: 0.2076\n",
            "Epoch 60/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2215 - val_loss: 0.2236\n",
            "Epoch 61/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2169 - val_loss: 0.2371\n",
            "Epoch 62/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2196 - val_loss: 0.2105\n",
            "Epoch 63/100\n",
            "13208/13208 [==============================] - 1s 108us/sample - loss: 0.2135 - val_loss: 0.2143\n",
            "Epoch 64/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2184 - val_loss: 0.2143\n",
            "Epoch 65/100\n",
            "13208/13208 [==============================] - 2s 121us/sample - loss: 0.2196 - val_loss: 0.2195\n",
            "Epoch 66/100\n",
            "13208/13208 [==============================] - 2s 123us/sample - loss: 0.2153 - val_loss: 0.2182\n",
            "Epoch 67/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2186 - val_loss: 0.2146\n",
            "Epoch 68/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2175 - val_loss: 0.2124\n",
            "Epoch 69/100\n",
            "13208/13208 [==============================] - 2s 120us/sample - loss: 0.2152 - val_loss: 0.2056\n",
            "Epoch 70/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2169 - val_loss: 0.2208\n",
            "Epoch 71/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2139 - val_loss: 0.2177\n",
            "Epoch 72/100\n",
            "13208/13208 [==============================] - 1s 108us/sample - loss: 0.2163 - val_loss: 0.2309\n",
            "Epoch 73/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2177 - val_loss: 0.2213\n",
            "Epoch 74/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2142 - val_loss: 0.2088\n",
            "Epoch 75/100\n",
            "13208/13208 [==============================] - 2s 118us/sample - loss: 0.2160 - val_loss: 0.2184\n",
            "Epoch 76/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2159 - val_loss: 0.2140\n",
            "Epoch 77/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2137 - val_loss: 0.2094\n",
            "Epoch 78/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2138 - val_loss: 0.2142\n",
            "Epoch 79/100\n",
            "13208/13208 [==============================] - 2s 121us/sample - loss: 0.2145 - val_loss: 0.2248\n",
            "Epoch 80/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2127 - val_loss: 0.2029\n",
            "Epoch 81/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2155 - val_loss: 0.2266\n",
            "Epoch 82/100\n",
            "13208/13208 [==============================] - 1s 112us/sample - loss: 0.2103 - val_loss: 0.2163\n",
            "Epoch 83/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2171 - val_loss: 0.2190\n",
            "Epoch 84/100\n",
            "13208/13208 [==============================] - 2s 120us/sample - loss: 0.2094 - val_loss: 0.2121\n",
            "Epoch 85/100\n",
            "13208/13208 [==============================] - 2s 121us/sample - loss: 0.2074 - val_loss: 0.2120\n",
            "Epoch 86/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2152 - val_loss: 0.2276\n",
            "Epoch 87/100\n",
            "13208/13208 [==============================] - 2s 115us/sample - loss: 0.2091 - val_loss: 0.2062\n",
            "Epoch 88/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2178 - val_loss: 0.2189\n",
            "Epoch 89/100\n",
            "13208/13208 [==============================] - 2s 117us/sample - loss: 0.2084 - val_loss: 0.2095\n",
            "Epoch 90/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2075 - val_loss: 0.2049\n",
            "Epoch 91/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2127 - val_loss: 0.2173\n",
            "Epoch 92/100\n",
            "13208/13208 [==============================] - 1s 111us/sample - loss: 0.2108 - val_loss: 0.2108\n",
            "Epoch 93/100\n",
            "13208/13208 [==============================] - 1s 108us/sample - loss: 0.2106 - val_loss: 0.2271\n",
            "Epoch 94/100\n",
            "13208/13208 [==============================] - 2s 116us/sample - loss: 0.2170 - val_loss: 0.2069\n",
            "Epoch 95/100\n",
            "13208/13208 [==============================] - 1s 110us/sample - loss: 0.2122 - val_loss: 0.2099\n",
            "Epoch 96/100\n",
            "13208/13208 [==============================] - 2s 114us/sample - loss: 0.2126 - val_loss: 0.2074\n",
            "Epoch 97/100\n",
            "13208/13208 [==============================] - 1s 107us/sample - loss: 0.2056 - val_loss: 0.2045\n",
            "Epoch 98/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2088 - val_loss: 0.2079\n",
            "Epoch 99/100\n",
            "13208/13208 [==============================] - 1s 109us/sample - loss: 0.2090 - val_loss: 0.2136\n",
            "Epoch 100/100\n",
            "13208/13208 [==============================] - 1s 113us/sample - loss: 0.2099 - val_loss: 0.2049\n",
            "153.1992735862732\n",
            "NN MSE:0.2029142018867607\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             multiple                  360       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc multiple                  160       \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             multiple                  1230      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc multiple                  120       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             multiple                  930       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc multiple                  120       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             multiple                  930       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc multiple                  120       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             multiple                  310       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc multiple                  40        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             multiple                  11        \n",
            "=================================================================\n",
            "Total params: 4,331\n",
            "Trainable params: 4,051\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFTqzVqF8wju",
        "colab_type": "code",
        "outputId": "dd5c4f9d-3e0e-434e-f3de-7e1b5340ff9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cp_rank=20\n",
        "local_dim=75\n",
        "\n",
        "x_train_processed,x_val_processed,x_test_processed=local_feature_mapping(x_train,x_val,x_test,local_dim=local_dim,unit_norm=True)\n",
        "\n",
        "print('rank: {}, local dimension:{}'.format(cp_rank,local_dim))\n",
        "\n",
        "model=Sequential()\n",
        "model.add(CP_Based(units=1,activation=None,cp_rank=cp_rank,local_dim=local_dim,regularizer=keras.regularizers.l2(3e-7),\n",
        "\n",
        "                    initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.2)))\n",
        "# ,regularizer=keras.regularizers.l2(2e-7),\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "start = time.time()\n",
        "history=model.fit(x_train_processed, y_train, epochs=100, batch_size=32,validation_data=(x_val_processed,y_val))\n",
        "end = time.time()\n",
        "\n",
        "train_loss=min(history.history['loss'])\n",
        "val_loss=min(history.history['val_loss'])\n",
        "print('train loss:{}'.format(train_loss))\n",
        "print('validation loss:{}'.format(val_loss))\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "   \n",
        "print('Test MSE:{}'.format(model.evaluate(x_test_processed,y_test)))\n",
        "print('Training time:{}'.format(end-start))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rank: 20, local dimension:75\n",
            "WARNING:tensorflow:Entity <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f91140c4780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f91140c4780>>, which Python reported as:\n",
            "    def call(self, X):\n",
            "            \n",
            "#        feat_tensor = tf.transpose([1-X,X],perm=[1, 2, 0]) # for images \n",
            "#        feat_tensor = tf.transpose([tf.sqrt(1-X**2),X],perm=[1, 2, 0]) # fpr regression\n",
            "#        X_square=X**2\n",
            "\n",
            "#        X_square=(X_square-tf.reduce_mean(X_square,axis=0,keepdims=True))/tf.math.reduce_std(X_square,axis=0,keepdims=True)\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X,X_square],perm=[1, 2, 0]) # fpr regression\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X],perm=[1, 2, 0]) # fpr regression\n",
            "        feat_tensor=X\n",
            "        \n",
            "        output_list=[]\n",
            "        \n",
            "        for unit in range(0,self.units):                 \n",
            "                    \n",
            "            feat_tensor_reshaped=tf.transpose(feat_tensor,perm=[1,0,2]) # NxNtxd\n",
            "            weights=tf.transpose(self.kernel[:,:,:,unit],perm=[2,0,1]) # Nxdxm\n",
            "            test=tf.matmul(feat_tensor_reshaped,weights) # NxNtxm\n",
            "            test_hadamard=tf.reduce_prod(test,axis=0) # Ntxm   \n",
            "            output=tf.reduce_sum(test_hadamard, axis=1) # Ntx1\n",
            "            output_list.append(output)\n",
            "        \n",
            "        to_return=tf.stack(output_list, axis=1)\n",
            "        return self.activation(to_return)\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in square\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f91140c4780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method AllOrder.call of <all_order_fast_with_reg.AllOrder object at 0x7f91140c4780>>, which Python reported as:\n",
            "    def call(self, X):\n",
            "            \n",
            "#        feat_tensor = tf.transpose([1-X,X],perm=[1, 2, 0]) # for images \n",
            "#        feat_tensor = tf.transpose([tf.sqrt(1-X**2),X],perm=[1, 2, 0]) # fpr regression\n",
            "#        X_square=X**2\n",
            "\n",
            "#        X_square=(X_square-tf.reduce_mean(X_square,axis=0,keepdims=True))/tf.math.reduce_std(X_square,axis=0,keepdims=True)\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X,X_square],perm=[1, 2, 0]) # fpr regression\n",
            "#        feat_tensor = tf.transpose([tf.ones(tf.shape(X)),X],perm=[1, 2, 0]) # fpr regression\n",
            "        feat_tensor=X\n",
            "        \n",
            "        output_list=[]\n",
            "        \n",
            "        for unit in range(0,self.units):                 \n",
            "                    \n",
            "            feat_tensor_reshaped=tf.transpose(feat_tensor,perm=[1,0,2]) # NxNtxd\n",
            "            weights=tf.transpose(self.kernel[:,:,:,unit],perm=[2,0,1]) # Nxdxm\n",
            "            test=tf.matmul(feat_tensor_reshaped,weights) # NxNtxm\n",
            "            test_hadamard=tf.reduce_prod(test,axis=0) # Ntxm   \n",
            "            output=tf.reduce_sum(test_hadamard, axis=1) # Ntx1\n",
            "            output_list.append(output)\n",
            "        \n",
            "        to_return=tf.stack(output_list, axis=1)\n",
            "        return self.activation(to_return)\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "Train on 13208 samples, validate on 3304 samples\n",
            "Epoch 1/100\n",
            "13208/13208 [==============================] - 2s 133us/sample - loss: 0.9028 - val_loss: 0.5711\n",
            "Epoch 2/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.4519 - val_loss: 0.3576\n",
            "Epoch 3/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.3212 - val_loss: 0.2961\n",
            "Epoch 4/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.2814 - val_loss: 0.2730\n",
            "Epoch 5/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.2623 - val_loss: 0.2599\n",
            "Epoch 6/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.2510 - val_loss: 0.2498\n",
            "Epoch 7/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.2411 - val_loss: 0.2445\n",
            "Epoch 8/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.2339 - val_loss: 0.2400\n",
            "Epoch 9/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.2282 - val_loss: 0.2312\n",
            "Epoch 10/100\n",
            "13208/13208 [==============================] - 1s 93us/sample - loss: 0.2228 - val_loss: 0.2283\n",
            "Epoch 11/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.2185 - val_loss: 0.2258\n",
            "Epoch 12/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.2155 - val_loss: 0.2236\n",
            "Epoch 13/100\n",
            "13208/13208 [==============================] - 1s 91us/sample - loss: 0.2130 - val_loss: 0.2228\n",
            "Epoch 14/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.2103 - val_loss: 0.2194\n",
            "Epoch 15/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.2085 - val_loss: 0.2178\n",
            "Epoch 16/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.2058 - val_loss: 0.2144\n",
            "Epoch 17/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.2041 - val_loss: 0.2157\n",
            "Epoch 18/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.2025 - val_loss: 0.2107\n",
            "Epoch 19/100\n",
            "13208/13208 [==============================] - 1s 96us/sample - loss: 0.2009 - val_loss: 0.2117\n",
            "Epoch 20/100\n",
            "13208/13208 [==============================] - 1s 93us/sample - loss: 0.1996 - val_loss: 0.2127\n",
            "Epoch 21/100\n",
            "13208/13208 [==============================] - 1s 95us/sample - loss: 0.1983 - val_loss: 0.2079\n",
            "Epoch 22/100\n",
            "13208/13208 [==============================] - 1s 93us/sample - loss: 0.1964 - val_loss: 0.2082\n",
            "Epoch 23/100\n",
            "13208/13208 [==============================] - 1s 91us/sample - loss: 0.1957 - val_loss: 0.2071\n",
            "Epoch 24/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.1948 - val_loss: 0.2082\n",
            "Epoch 25/100\n",
            "13208/13208 [==============================] - 1s 85us/sample - loss: 0.1934 - val_loss: 0.2042\n",
            "Epoch 26/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1925 - val_loss: 0.2055\n",
            "Epoch 27/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.1921 - val_loss: 0.2038\n",
            "Epoch 28/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1908 - val_loss: 0.2042\n",
            "Epoch 29/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1899 - val_loss: 0.2047\n",
            "Epoch 30/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1887 - val_loss: 0.2122\n",
            "Epoch 31/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1889 - val_loss: 0.2031\n",
            "Epoch 32/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1880 - val_loss: 0.2021\n",
            "Epoch 33/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1872 - val_loss: 0.2022\n",
            "Epoch 34/100\n",
            "13208/13208 [==============================] - 1s 91us/sample - loss: 0.1870 - val_loss: 0.2004\n",
            "Epoch 35/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1862 - val_loss: 0.1994\n",
            "Epoch 36/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1857 - val_loss: 0.1996\n",
            "Epoch 37/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1855 - val_loss: 0.2017\n",
            "Epoch 38/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1848 - val_loss: 0.2018\n",
            "Epoch 39/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1840 - val_loss: 0.2022\n",
            "Epoch 40/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1837 - val_loss: 0.1986\n",
            "Epoch 41/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1830 - val_loss: 0.1995\n",
            "Epoch 42/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1832 - val_loss: 0.2028\n",
            "Epoch 43/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.1824 - val_loss: 0.1989\n",
            "Epoch 44/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1821 - val_loss: 0.2002\n",
            "Epoch 45/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1817 - val_loss: 0.1988\n",
            "Epoch 46/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1814 - val_loss: 0.1999\n",
            "Epoch 47/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1811 - val_loss: 0.1991\n",
            "Epoch 48/100\n",
            "13208/13208 [==============================] - 1s 93us/sample - loss: 0.1808 - val_loss: 0.2001\n",
            "Epoch 49/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1800 - val_loss: 0.2064\n",
            "Epoch 50/100\n",
            "13208/13208 [==============================] - 1s 91us/sample - loss: 0.1797 - val_loss: 0.1984\n",
            "Epoch 51/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1795 - val_loss: 0.1999\n",
            "Epoch 52/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1794 - val_loss: 0.2008\n",
            "Epoch 53/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1785 - val_loss: 0.2005\n",
            "Epoch 54/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1792 - val_loss: 0.1993\n",
            "Epoch 55/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1780 - val_loss: 0.1985\n",
            "Epoch 56/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1785 - val_loss: 0.1999\n",
            "Epoch 57/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1779 - val_loss: 0.1972\n",
            "Epoch 58/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1775 - val_loss: 0.1986\n",
            "Epoch 59/100\n",
            "13208/13208 [==============================] - 1s 91us/sample - loss: 0.1772 - val_loss: 0.2062\n",
            "Epoch 60/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.1771 - val_loss: 0.1976\n",
            "Epoch 61/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1770 - val_loss: 0.1993\n",
            "Epoch 62/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1764 - val_loss: 0.1980\n",
            "Epoch 63/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.1764 - val_loss: 0.1987\n",
            "Epoch 64/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1756 - val_loss: 0.1974\n",
            "Epoch 65/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1755 - val_loss: 0.2030\n",
            "Epoch 66/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1757 - val_loss: 0.1988\n",
            "Epoch 67/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1754 - val_loss: 0.1982\n",
            "Epoch 68/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1754 - val_loss: 0.1961\n",
            "Epoch 69/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1752 - val_loss: 0.2036\n",
            "Epoch 70/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1749 - val_loss: 0.1965\n",
            "Epoch 71/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1748 - val_loss: 0.2028\n",
            "Epoch 72/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1742 - val_loss: 0.1995\n",
            "Epoch 73/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1741 - val_loss: 0.1979\n",
            "Epoch 74/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.1739 - val_loss: 0.1965\n",
            "Epoch 75/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1734 - val_loss: 0.1979\n",
            "Epoch 76/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1734 - val_loss: 0.1960\n",
            "Epoch 77/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1732 - val_loss: 0.2032\n",
            "Epoch 78/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1727 - val_loss: 0.1983\n",
            "Epoch 79/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1728 - val_loss: 0.2005\n",
            "Epoch 80/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1734 - val_loss: 0.2013\n",
            "Epoch 81/100\n",
            "13208/13208 [==============================] - 1s 92us/sample - loss: 0.1730 - val_loss: 0.2009\n",
            "Epoch 82/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1721 - val_loss: 0.2005\n",
            "Epoch 83/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1721 - val_loss: 0.1978\n",
            "Epoch 84/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1716 - val_loss: 0.2011\n",
            "Epoch 85/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1720 - val_loss: 0.1971\n",
            "Epoch 86/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1714 - val_loss: 0.1998\n",
            "Epoch 87/100\n",
            "13208/13208 [==============================] - 1s 89us/sample - loss: 0.1719 - val_loss: 0.1986\n",
            "Epoch 88/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1710 - val_loss: 0.2011\n",
            "Epoch 89/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1712 - val_loss: 0.2007\n",
            "Epoch 90/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1710 - val_loss: 0.1979\n",
            "Epoch 91/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1709 - val_loss: 0.1983\n",
            "Epoch 92/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1706 - val_loss: 0.1993\n",
            "Epoch 93/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1702 - val_loss: 0.2027\n",
            "Epoch 94/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1701 - val_loss: 0.1980\n",
            "Epoch 95/100\n",
            "13208/13208 [==============================] - 1s 90us/sample - loss: 0.1697 - val_loss: 0.1996\n",
            "Epoch 96/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.1702 - val_loss: 0.1992\n",
            "Epoch 97/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1704 - val_loss: 0.2022\n",
            "Epoch 98/100\n",
            "13208/13208 [==============================] - 1s 88us/sample - loss: 0.1699 - val_loss: 0.2004\n",
            "Epoch 99/100\n",
            "13208/13208 [==============================] - 1s 87us/sample - loss: 0.1700 - val_loss: 0.1987\n",
            "Epoch 100/100\n",
            "13208/13208 [==============================] - 1s 86us/sample - loss: 0.1697 - val_loss: 0.1989\n",
            "train loss:0.1697025567491729\n",
            "validation loss:0.1959992485170503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEvCAYAAAB2Xan3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4128/4128 [==============================] - 0s 44us/sample - loss: 0.1980\n",
            "Test MSE:0.19802677287727363\n",
            "Training time:118.84296131134033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f09BoyTA7q4a",
        "colab_type": "code",
        "outputId": "0aed23c4-8015-4ee4-98cb-1f2abca1b4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "all_order_7 (AllOrder)       multiple                  4000      \n",
            "=================================================================\n",
            "Total params: 4,000\n",
            "Trainable params: 4,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}